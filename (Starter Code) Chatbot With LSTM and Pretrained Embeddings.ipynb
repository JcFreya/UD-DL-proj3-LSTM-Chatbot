{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==1.12.0 in /root/.local/lib/python3.7/site-packages (1.12.0)\n",
      "Requirement already satisfied: torchdata==0.4.0 in /root/.local/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: torchtext==0.13.0 in /root/.local/lib/python3.7/site-packages (0.13.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.0) (3.7.4.1)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /root/.local/lib/python3.7/site-packages (from torchdata==0.4.0) (2.7.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata==0.4.0) (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.4.0) (1.25.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.13.0) (4.43.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.13.0) (1.21.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.4.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.4.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.4.0) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "# download liabries needed\n",
    "# need to restart the kernal after running this cell\n",
    "!pip install torch==1.12.0 torchdata==0.4.0 torchtext==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds\n",
    "SEED=42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build Vocabulary & create the Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "# function to load the data\n",
    "def loadDF(path):\n",
    "    \n",
    "    '''\n",
    "    You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "    Number of lines per split:\n",
    "        train: 87599\n",
    "        dev: 10570\n",
    "    '''\n",
    "    # load data\n",
    "    train_data, valid_data = torchtext.datasets.SQuAD1(root=path, split=('train', 'dev'))\n",
    "    \n",
    "    # returns: DataPipe that yields data points from SQuaAD1 dataset which consist of context, question, \n",
    "    # list of answers and corresponding index in context\n",
    "    # convert dataPipe to dictionary \n",
    "    # make simple pairs of questions and answers\n",
    "#     train_dict = {'src':[], 'trg':[]}\n",
    "#     valid_dict = {'src':[], 'trg':[]}\n",
    "    \n",
    "    \n",
    "    train_dict = [{'src': question, 'trg': answers[0]} for _, question, answers, _ in train_data]\n",
    "    valid_dict = [{'src': question, 'trg': answers[0]} for _, question, answers, _ in val_data]\n",
    "    \n",
    "#     for _, question, answer, _ in train_data:\n",
    "#         train_dict['src'].append(question)\n",
    "#         train_dict['trg'].append(answer[0])\n",
    "    \n",
    "#     for _, question, answer, _ in valid_data:\n",
    "#         valid_dict['src'].append(question)\n",
    "#         valid_dict['trg'].append(answers[0])\n",
    "        \n",
    "    # convert Dictionaries to Pandas DataFrame\n",
    "    train_df = pd.DataFrame(train_dict)    \n",
    "    valid_df = pd.DataFrame(valid_dict)\n",
    "    \n",
    "    # combine two parts\n",
    "    # df = train_df.append(validation_df)\n",
    "    \n",
    "    return train_df, valid_df\n",
    "#     return train_dict, valid_dict\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    \n",
    "    '''\n",
    "    Our text needs to be cleaned with a tokenizer. This function will perform that task.\n",
    "    https://www.nltk.org/api/nltk.tokenize.html\n",
    "    '''\n",
    "    # clean text\n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    \n",
    "    stemmer = snowball.SnowballStemmer('english')\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def train_test_split(SRC, TRG):\n",
    "    \n",
    "    '''\n",
    "    Input: SRC, our list of questions from the dataset\n",
    "            TRG, our list of responses from the dataset\n",
    "\n",
    "    Output: Training and test datasets for SRC & TRG\n",
    "\n",
    "    '''\n",
    "    SRC_train_dataset = SRC.sample(frac=0.8, random_state=SEED)\n",
    "    SRC_test_dataset = SRC.drop(SRC_train_dataset.index)\n",
    "\n",
    "    TRG_train_dataset = TRG.sample(frac=0.8, random_state=SEED)\n",
    "    TRG_test_dataset = TRG.drop(TRG_train_dataset.index)\n",
    "    \n",
    "    return SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loadDF function\n",
    "train_raw, valid_raw = loadDF('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make implementation test quicker, grab a subset of whole dataset\n",
    "# data_df = data_df_raw.iloc[:5000, :]\n",
    "train_data = train_raw.iloc[:5000, :]\n",
    "valid_data = valid_raw.iloc[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                       trg  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top rows of train data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set: (87599, 2)\n",
      "valid data set: (10570, 2)\n"
     ]
    }
   ],
   "source": [
    "# count of train_df_raw, valid_df_raw\n",
    "print(f'train data set: {train_raw.shape}')\n",
    "print(f'valid data set: {valid_raw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[to, whom, did, the, virgin, mari, alleg, appe...</td>\n",
       "      <td>[saint, bernadett, soubir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what, is, in, front, of, the, notr, dame, mai...</td>\n",
       "      <td>[a, copper, statu, of, christ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, basilica, of, the, sacr, heart, at, notr...</td>\n",
       "      <td>[the, main, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[what, is, the, grotto, at, notr, dame]</td>\n",
       "      <td>[a, marian, place, of, prayer, and, reflect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[what, sit, on, top, of, the, main, build, at,...</td>\n",
       "      <td>[a, golden, statu, of, the, virgin, mari]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  [to, whom, did, the, virgin, mari, alleg, appe...   \n",
       "1  [what, is, in, front, of, the, notr, dame, mai...   \n",
       "2  [the, basilica, of, the, sacr, heart, at, notr...   \n",
       "3            [what, is, the, grotto, at, notr, dame]   \n",
       "4  [what, sit, on, top, of, the, main, build, at,...   \n",
       "\n",
       "                                            trg  \n",
       "0                    [saint, bernadett, soubir]  \n",
       "1                [a, copper, statu, of, christ]  \n",
       "2                            [the, main, build]  \n",
       "3  [a, marian, place, of, prayer, and, reflect]  \n",
       "4     [a, golden, statu, of, the, virgin, mari]  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the test\n",
    "train_data.loc[:, 'src'] = train_data['src'].apply(prepare_text)\n",
    "train_data.loc[:, 'trg'] = train_data['trg'].apply(prepare_text)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[which, nfl, team, repres, the, afc, at, super...</td>\n",
       "      <td>[denver, bronco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[which, nfl, team, repres, the, nfc, at, super...</td>\n",
       "      <td>[carolina, panther]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[where, did, super, bowl, 50, take, place]</td>\n",
       "      <td>[santa, clara, california]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[which, nfl, team, won, super, bowl, 50]</td>\n",
       "      <td>[denver, bronco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[what, color, was, use, to, emphas, the, 50th,...</td>\n",
       "      <td>[gold]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  [which, nfl, team, repres, the, afc, at, super...   \n",
       "1  [which, nfl, team, repres, the, nfc, at, super...   \n",
       "2         [where, did, super, bowl, 50, take, place]   \n",
       "3           [which, nfl, team, won, super, bowl, 50]   \n",
       "4  [what, color, was, use, to, emphas, the, 50th,...   \n",
       "\n",
       "                          trg  \n",
       "0            [denver, bronco]  \n",
       "1         [carolina, panther]  \n",
       "2  [santa, clara, california]  \n",
       "3            [denver, bronco]  \n",
       "4                      [gold]  "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.loc[:, 'src'] = valid_data['src'].apply(prepare_text)\n",
    "valid_data.loc[:, 'trg'] = valid_data['trg'].apply(prepare_text)\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train and test data\n",
    "SRC = train_data[['src']]\n",
    "TRG = train_data[['trg']]\n",
    "SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset = train_test_split(SRC, TRG)\n",
    "\n",
    "SRC_valid_dataset = valid_data[['src']]\n",
    "TRG_valid_dataset = valid_data[['trg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_train_dataset size: (4000, 1)\n",
      "SRC_test_dataset size: (1000, 1)\n",
      "SRC_valid_dataset size: (5000, 1)\n",
      "TRG_train_dataset size: (4000, 1)\n",
      "TRG_test_dataset size: (1000, 1)\n",
      "TRG_valid_dataset size: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'SRC_train_dataset size: {SRC_train_dataset.shape}')\n",
    "print(f'SRC_test_dataset size: {SRC_test_dataset.shape}')\n",
    "print(f'SRC_valid_dataset size: {SRC_valid_dataset.shape}')\n",
    "\n",
    "print(f'TRG_train_dataset size: {TRG_train_dataset.shape}')\n",
    "print(f'TRG_test_dataset size: {TRG_test_dataset.shape}')\n",
    "print(f'TRG_valid_dataset size: {TRG_valid_dataset.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>[what, year, did, chopin, die]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>[what, appl, code, name, for, the, newer, 8pin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>[on, what, devic, can, video, game, be, use]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>[where, doe, the, saskatchewan, river, empti, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>[who, coach, beyoncé, for, her, spanish, record]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src\n",
       "1501                     [what, year, did, chopin, die]\n",
       "2586  [what, appl, code, name, for, the, newer, 8pin...\n",
       "2653       [on, what, devic, can, video, game, be, use]\n",
       "1055  [where, doe, the, saskatchewan, river, empti, ...\n",
       "705    [who, coach, beyoncé, for, her, spanish, record]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of SRC_train_dataset\n",
    "SRC_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>[1849]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>[lightn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>[ipod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>[hudson, bay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>[rudi, perez]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                trg\n",
       "1501         [1849]\n",
       "2586       [lightn]\n",
       "2653         [ipod]\n",
       "1055  [hudson, bay]\n",
       "705   [rudi, perez]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of TRG_train_dataset\n",
    "TRG_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Vovabulary Object\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        SOS = 0\n",
    "        EOS = 1\n",
    "\n",
    "        self.name = name\n",
    "        self.index = {SOS: \"\", EOS: \"\"}\n",
    "        self.count = 0\n",
    "        self.words = {\"\": SOS, \"\": EOS}\n",
    "        \n",
    "    def indexWord(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words[word] = self.count\n",
    "            self.index[str(self.count)] = word\n",
    "            self.count += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabularies for questions \"source\" and answers \"target\"\n",
    "vocab_src = Vocab(name='src')\n",
    "vocab_trg = Vocab(name='trg')\n",
    "\n",
    "for idx, r in train_data.iterrows():\n",
    "    for w in r['src']:\n",
    "        vocab_src.indexWord(w)\n",
    "    for w in r['trg']:\n",
    "        vocab_trg.indexWord(w)\n",
    "        \n",
    "for idx, r in valid_data.iterrows():\n",
    "    for w in r['src']:\n",
    "        vocab_src.indexWord(w)\n",
    "    for w in r['trg']:\n",
    "        vocab_trg.indexWord(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens in vocab_src: 7041\n",
      "tokens in vocab_trg: 7032\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokens in vocab_src: {vocab_src.count}\")\n",
    "print(f\"tokens in vocab_trg: {vocab_trg.count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/iJoud/Seq2Seq-Chatbot/blob/main/src/Data.py\n",
    "def toTensor(vocab, sentence):\n",
    "    # convert list of words \"sentence\" to a torch tensor of indices\n",
    "    indices = [vocab.words[word] for word in sentence]\n",
    "    indices.append(vocab.words[''])\n",
    "    return torch.Tensor(indices).long().to(device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_train_dataset = [toTensor(vocab_src, r['src']) for idx, r in SRC_train_dataset.iterrows()]\n",
    "TRG_train_dataset = [toTensor(vocab_trg, r['trg']) for idx, r in TRG_train_dataset.iterrows()]\n",
    "SRC_test_dataset = [toTensor(vocab_src, r['src']) for idx, r in SRC_test_dataset.iterrows()]\n",
    "TRG_test_dataset = [toTensor(vocab_trg, r['trg']) for idx, r in TRG_test_dataset.iterrows()]\n",
    "SRC_valid_dataset = [toTensor(vocab_src, r['src']) for idx, r in SRC_valid_dataset.iterrows()]\n",
    "TRG_valid_dataset = [toTensor(vocab_trg, r['trg']) for idx, r in TRG_valid_dataset.iterrows()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Encoder\n",
    "The Encoder's job is to create a representation of the input sequence. Then, it captures the representation in the hidden state of the LSTM. And finally, it passes the hidden state to the second half of Seq2Seq.\n",
    "\n",
    "The layers of The Encoder are:\n",
    "- The Embedding Layer\n",
    "- The LSTM\n",
    "- Dropout Layer (optional)\n",
    "\n",
    "The parameters of The Encoder are:\n",
    "\n",
    "- The input size\n",
    "- The hidden size\n",
    "- The embedding size\n",
    "\n",
    "ref:\n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a torch.device. This is used to tell torchText to put the tensors on the GPU or not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "# Create the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, n_layers, dropout):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, i):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        #Inputs = [input_size, batch size]\n",
    "        embedded = self.dropout(self.embedding(i))\n",
    "        \n",
    "        #embedded = [input_size, batch size, embedding_size]\n",
    "        o, (h, c) = self.lstm(embedded)\n",
    "        \n",
    "        #outputs = [input_size, batch size, hidden_size * n directions]\n",
    "        #hidden = [n_layers * n directions, batch size, hidden_size]\n",
    "        #cell = [n_layers * n directions, batch size, hidden_size]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return o, h, c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Decoder\n",
    "The Decoder's job is to output a prediction based on the hidden state of The Encoder. It combines this with information from the N-1 prediction to create an output.\n",
    "\n",
    "The layers of The Decoder are:\n",
    "\n",
    "- The Embedding Layer\n",
    "- The LSTM\n",
    "- The Linear Output Layer\n",
    "\n",
    "The parameters of The Encoder are:\n",
    "\n",
    "- The output size\n",
    "- The hidden size\n",
    "- The embedding size\n",
    "\n",
    "ref: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Decoder\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size, n_layers, dropout):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden stat\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, n_layers, dropout = dropout)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer     \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, i, h, c):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        #input = [batch size]\n",
    "        #hidden = [n_layers * n directions, batch size, hidden_size]\n",
    "        #cell = [n_layers * n directions, batch size, hidden_size]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n_layers, batch size, hidden_size]\n",
    "        #context = [n_layerss, batch size, hidden_size]\n",
    "        \n",
    "        i = i.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(i))\n",
    "        #embedded = [1, batch size, embedding_size]\n",
    "        \n",
    "        output, (h, c) = self.lstm(embedded, (h, c))\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        o = self.out(output.squeeze(0))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return o, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine into a Seq2Seq Architecture\n",
    "This will handle:\n",
    "\n",
    "- receiving the input/source sentence\n",
    "- using the encoder to produce the context vectors\n",
    "- using the decoder to produce the predicted output/target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a Seq2Seq Architecture\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_size, encoder_hidden_size, encoder_embedding_size, encoder_n_layers, encoder_dropout,\\\n",
    "                 decoder_hidden_size, decoder_output_size, decoder_embedding_size, decoder_n_layers, decoder_dropout):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(encoder_input_size, encoder_hidden_size, encoder_embedding_size, encoder_n_layers, encoder_dropout)\n",
    "        self.decoder = Decoder(decoder_hidden_size, decoder_output_size, decoder_embedding_size, decoder_n_layers, decoder_dropout)\n",
    "    \n",
    "    def forward(self, src, trg, batch_size, teacher_forcing_ratio = 0.5):      \n",
    "#     def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "#         batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "#         trg_len = trg.size(0)\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        o = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        encoder_output, h, c = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        decoder_input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states            \n",
    "            decoder_output, h = self.decoder(decoder_input, h, c)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            o[t] = decoder_output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = decoder_output.argmax(1)\n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            decoder_input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return o\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train & evaluate model\n",
    "\n",
    "ref: https://learn.udacity.com/nanodegrees/nd101/parts/cd1822/lessons/23e85aa3-ecde-4dc6-ae90-dc7144383206/concepts/e1424b2b-4627-4953-b342-78b4c444478a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nitialize the model\n",
    "# the input and output dimensions are defined by the size of the vocabulary\n",
    "# the embedding dimesions and dropout for the encoder and decoder can be different, \n",
    "# but the size of the hidden/cell states must be the same.\n",
    "# then define the encoder, decoder and then Seq2Seq model, which we place on the device.\n",
    "\n",
    "encoder_input_size = vocab_src.count\n",
    "decoder_output_size = vocab_trg.count\n",
    "encoder_embedding_size = 256\n",
    "decoder_embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "model = Seq2Seq(encoder_input_size, hidden_size, encoder_embedding_size, n_layers, encoder_dropout,\\\n",
    "                 hidden_size, decoder_output_size, decoder_embedding_size, n_layers, decoder_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer, which we use to update our parameters in the training loop. \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, source_data, target_data, batch_size, optimizer, criterion, clip):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(0, len(source_data)):\n",
    "        \n",
    "        src = source_data[i]\n",
    "        trg = target_data[i]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg, batch_size, 0.5)\n",
    "        \n",
    "        #trg = [trg len, batch_size]\n",
    "        #output = [trg len, batch_size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate function\n",
    "def evaluate(model, source_data, target_data, batch_size, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(0, len(source_data)):\n",
    "            \n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, batch_size, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch_size]\n",
    "            #output = [trg len, batch_size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch_size]\n",
    "            #output = [(trg len - 1) * batch_size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (128) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-341-df1243d4d314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRC_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-339-a7032ae77afa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, source_data, target_data, batch_size, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#output = [(trg len - 1) * batch size, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (128) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# At each epoch, will check if model has achieved the best validation loss so far. \n",
    "# If it has, will update best validation loss and save the parameters of our model. \n",
    "# when test model, will use the saved parameters used to achieve the best validation loss.\n",
    "\n",
    "n_epochs = 1\n",
    "clip = 1\n",
    "batch_size = 128\n",
    "model_path = 'seq2seq.pt'\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = train(model, SRC_train_dataset, TRG_train_dataset, batch_size, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, source_data, target_data, batch_size, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f}')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interact with the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple interactive interfaces\n",
    "print(\"Type 'stop' to exit chat\")\n",
    "ANSWER_LENGTH = 12\n",
    "while True:\n",
    "    src = input(\">\")\n",
    "    \n",
    "    # If STOP in input, stop script\n",
    "    if \"stop\" == src.strip():\n",
    "        break\n",
    "    # get the answer\n",
    "    src = toTensor(vocab_src, \" \".join(prepare_text(src)))\n",
    "    \n",
    "    answer_words = [] \n",
    "    output = model(src, trg, batch_size, 0)\n",
    "    \n",
    "    for tensor in output['decoder_output']:\n",
    "\n",
    "        _, top_token = tensor.data.topk(1)\n",
    "        if top_token.item() == 1:\n",
    "            break\n",
    "        else:\n",
    "            word = vocab_trg.index[top_token.item()]\n",
    "            answer_words.append(word)\n",
    "\n",
    "    # write out an answer for user\n",
    "    print(\"<\", \" \".join(answer), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
